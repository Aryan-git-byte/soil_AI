{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e59601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "#   FARM-BOT URL ‚Üí PDF ‚Üí CLEAN ‚Üí EMBED ‚Üí QDRANT PIPELINE (STRICT INPUT ONLY)\n",
    "# ======================================\n",
    "\n",
    "!pip install sentence-transformers qdrant-client pymupdf tqdm\n",
    "\n",
    "import requests\n",
    "import uuid\n",
    "import fitz\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "QDRANT_API_KEY=\"your key here\"\n",
    "QDRANT_URL=\"your key here\"\n",
    "COLLECTION = \"farmbot_knowledge\"\n",
    "EMBEDDING_DIM = 384\n",
    "\n",
    "# ---------------------------\n",
    "# USER INPUT (NO DEFAULTS!)\n",
    "# ---------------------------\n",
    "print(\"Paste your PDF links below (one per line).\")\n",
    "print(\"Nothing will run unless you paste links.\\n\")\n",
    "\n",
    "raw_input_links = input(\"Links:\\n\").strip()\n",
    "\n",
    "# If user enters nothing ‚Üí empty list\n",
    "if raw_input_links == \"\":\n",
    "    print(\"\\n‚ùå No links provided. Nothing to process.\")\n",
    "    links = []\n",
    "else:\n",
    "    links = [u.strip() for u in raw_input_links.split(\"\\n\") if u.strip()]\n",
    "\n",
    "print(f\"\\nüìå Total URLs to process: {len(links)}\")\n",
    "for l in links:\n",
    "    print(\" ‚Üí\", l)\n",
    "\n",
    "if len(links) == 0:\n",
    "    raise SystemExit(\"‚ö†Ô∏è Exiting. No URLs to process.\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# MODEL & QDRANT CLIENT\n",
    "# ---------------------------\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "qdrant = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "\n",
    "# Ensure collection exists\n",
    "try:\n",
    "    qdrant.get_collection(COLLECTION)\n",
    "except:\n",
    "    qdrant.create_collection(\n",
    "        collection_name=COLLECTION,\n",
    "        vectors_config=models.VectorParams(size=EMBEDDING_DIM, distance=models.Distance.COSINE)\n",
    "    )\n",
    "    print(\"Created Qdrant collection:\", COLLECTION)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# HELPERS\n",
    "# ---------------------------\n",
    "\n",
    "def download_url(url):\n",
    "    \"\"\"Download PDF from URL.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        path = f\"/content/{uuid.uuid4()}.pdf\"\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Download failed for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def pdf_to_text(path):\n",
    "    \"\"\"Extract text from PDF using PyMuPDF.\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Text extraction failed for {path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def clean_text(text: str):\n",
    "    \"\"\"Clean extracted text.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
    "    text = re.sub(r'¬©.*?\\d+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def chunk_text(text, max_words=800):\n",
    "    \"\"\"Yield fixed-size text chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), max_words):\n",
    "        yield \" \".join(words[i:i + max_words])\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN PIPELINE\n",
    "# ---------------------------\n",
    "total_chunks = 0\n",
    "failed_urls = []\n",
    "\n",
    "for url in tqdm(links, desc=\"Processing URLs\"):\n",
    "\n",
    "    print(f\"\\n=== üåê Processing URL: {url} ===\")\n",
    "\n",
    "    # 1. Download\n",
    "    pdf_path = download_url(url)\n",
    "    if not pdf_path:\n",
    "        failed_urls.append(url)\n",
    "        continue\n",
    "\n",
    "    # 2. Extract + clean\n",
    "    raw = pdf_to_text(pdf_path)\n",
    "    if not raw.strip():\n",
    "        print(\"‚ö†Ô∏è No text extracted. Skipping.\")\n",
    "        failed_urls.append(url)\n",
    "        continue\n",
    "\n",
    "    cleaned = clean_text(raw)\n",
    "\n",
    "    # 3. Chunk\n",
    "    chunks = list(chunk_text(cleaned))\n",
    "    print(f\"‚Üí Extracted {len(chunks)} chunks.\")\n",
    "\n",
    "    # 4. Batch embed\n",
    "    embeddings = model.encode(chunks, batch_size=16, convert_to_numpy=True)\n",
    "\n",
    "    # 5. Batch upload\n",
    "    points = []\n",
    "    for i, (vec, chunk) in enumerate(zip(embeddings, chunks)):\n",
    "        points.append(\n",
    "            models.PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vec.tolist(),\n",
    "                payload={\n",
    "                    \"source\": url,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"text\": chunk\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    qdrant.upsert(collection_name=COLLECTION, points=points)\n",
    "    print(f\"‚úî Uploaded {len(points)} chunks ‚Üí {COLLECTION}\")\n",
    "\n",
    "    total_chunks += len(points)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# SUMMARY\n",
    "# ---------------------------\n",
    "print(\"\\nüéâ DONE ‚Äî URL Pipeline Complete!\")\n",
    "print(f\"üß© Total chunks uploaded: {total_chunks}\")\n",
    "print(f\"üåê Total URLs processed: {len(links)}\")\n",
    "print(f\"‚ùå Failed URLs: {len(failed_urls)}\")\n",
    "\n",
    "if failed_urls:\n",
    "    print(\"\\nFailed list:\")\n",
    "    for u in failed_urls:\n",
    "        print(\" -\", u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eba93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant = QdrantClient(\n",
    "    url=\"your key here\",\n",
    "    api_key=\"your key here\")\n",
    "\n",
    "print(qdrant.get_collections())\n",
    "\n",
    "print(qdrant.count(collection_name=\"farmbot_knowledge\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ec0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "#   FARM-BOT ZIP ‚Üí PDF ‚Üí CLEAN ‚Üí EMBED ‚Üí QDRANT PIPELINE\n",
    "# ======================================\n",
    "\n",
    "!pip install sentence-transformers qdrant-client pymupdf tqdm\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import fitz\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "ZIP_PATH = \"/content/drive/MyDrive/Colab Notebooks/publication.zip\"\n",
    "EXTRACT_DIR = \"/content/publications_extracted\"\n",
    "\n",
    "QDRANT_API_KEY=\"Your key here\"\n",
    "QDRANT_URL=\"Your key here\"\n",
    "COLLECTION = \"farmbot_knowledge\"\n",
    "EMBEDDING_DIM = 384\n",
    "\n",
    "# ---------------------------\n",
    "# LOAD MODEL & QDRANT\n",
    "# ---------------------------\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "qdrant = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "\n",
    "# Ensure collection exists\n",
    "try:\n",
    "    qdrant.get_collection(COLLECTION)\n",
    "except:\n",
    "    qdrant.create_collection(\n",
    "        collection_name=COLLECTION,\n",
    "        vectors_config=models.VectorParams(size=EMBEDDING_DIM, distance=models.Distance.COSINE)\n",
    "    )\n",
    "    print(\"Created Qdrant collection:\", COLLECTION)\n",
    "\n",
    "# ---------------------------\n",
    "# HELPERS\n",
    "# ---------------------------\n",
    "\n",
    "def clean_text(text: str):\n",
    "    \"\"\"Clean extracted PDF text.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)          # fix spacing\n",
    "    text = re.sub(r'Page \\d+ of \\d+', '', text) \n",
    "    text = re.sub(r'¬©.*?\\d+', '', text)       # remove copyright junk\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text(text, max_words=800):\n",
    "    \"\"\"Yield word chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), max_words):\n",
    "        yield \" \".join(words[i:i + max_words])\n",
    "\n",
    "\n",
    "def pdf_to_text(path):\n",
    "    try:\n",
    "        doc = fitz.open(path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not extract: {path} ‚Äì {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 1) EXTRACT ZIP\n",
    "# ---------------------------\n",
    "print(\"üì¶ Extracting zip:\", ZIP_PATH)\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_DIR)\n",
    "    print(\"‚úÖ ZIP extracted successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå ZIP extraction failed:\", e)\n",
    "    raise\n",
    "\n",
    "# ---------------------------\n",
    "# 2) SCAN PDFs\n",
    "# ---------------------------\n",
    "pdf_files = []\n",
    "for root, dirs, files in os.walk(EXTRACT_DIR):\n",
    "    for f in files:\n",
    "        if f.lower().endswith(\".pdf\"):\n",
    "            pdf_files.append(os.path.join(root, f))\n",
    "\n",
    "print(f\"üîé Found {len(pdf_files)} PDF files.\\n\")\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN PIPELINE\n",
    "# ---------------------------\n",
    "total_chunks = 0\n",
    "failed_pdfs = []\n",
    "\n",
    "for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "\n",
    "    print(f\"\\n=== üìÑ Processing PDF: {pdf_path} ===\")\n",
    "\n",
    "    raw = pdf_to_text(pdf_path)\n",
    "    if not raw.strip():\n",
    "        print(\"‚ö†Ô∏è No text extracted. Skipping.\")\n",
    "        failed_pdfs.append(pdf_path)\n",
    "        continue\n",
    "\n",
    "    cleaned = clean_text(raw)\n",
    "\n",
    "    chunks = list(chunk_text(cleaned))\n",
    "    print(f\"‚Üí Extracted {len(chunks)} text chunks.\")\n",
    "\n",
    "    # Batch embed\n",
    "    embeddings = model.encode(chunks, batch_size=16, convert_to_numpy=True)\n",
    "\n",
    "    # Batch upload\n",
    "    points = []\n",
    "    for i, (vec, chunk) in enumerate(zip(embeddings, chunks)):\n",
    "        points.append(\n",
    "            models.PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vec.tolist(),\n",
    "                payload={\n",
    "                    \"source\": pdf_path,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"text\": chunk\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    qdrant.upsert(collection_name=COLLECTION, points=points)\n",
    "    print(f\"‚úî Uploaded {len(points)} chunks to Qdrant.\")\n",
    "\n",
    "    total_chunks += len(points)\n",
    "\n",
    "print(\"\\nüéâ ALL DONE!\")\n",
    "print(f\"üìö Total PDFs processed: {len(pdf_files)}\")\n",
    "print(f\"üß© Total chunks uploaded: {total_chunks}\")\n",
    "print(f\"‚ùå Failed PDFs: {len(failed_pdfs)}\")\n",
    "if failed_pdfs:\n",
    "    print(\"Failed list:\")\n",
    "    for f in failed_pdfs:\n",
    "        print(\" -\", f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
